* Semester 1
** Week 3
*** 06/10/17
**** Look at edit distance based metric
- weighted edited so that adding/removing weight from an edge counts as an edit in the distance metric
- Removing nodes/edges also counts to edit distance
- https://link.springer.com/content/pdf/10.1007%2Fs10044-008-0141-y.pdf

**** Transform graph to metric space
- Gromovâ€“Hausdorff
- Similar graphs give similar values
- Each node with edges it is connected to and their weights

**** Lobster graph as undirected tree
- Undirected tree is a tree where root is not known
- An undirected graph with no cycles is an undirected tree
- An find less computationally complex algorithm for undirected tree distance rather than graph distance

**** GUI tool to draw graphs and export to .dot files
- d3.js 
- networkx

**** Human pose graph matching papers
- Good ideas that have been tried or bad ones to avoid
- 

** Week 4

*** Before meetings
**** Human pose graph matching http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.592.4170&rep=rep1&type=pdf
- Graph matching algorithm used: Algorithm for error-tolerant subgraph isomorphism detection http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=682179
- Store database of model graphs. Graph matching algorithm could return multiple matches from database because it only looks at topology
- Decision trees used to further filter matches. Example graphs are created for decision to learn. 

**** Graph creation software
- Gephi to draw graphs, it can export drawn graphs into a number of data formats. 
- What is the issue with networkx/d3.js? Can't express edge/node relations?

*** 12/10/17
**** Create graphs from lobsters to try matching
- Have some sample graphs to be able to run through nauty and try the matching
- augment hand made graphs by adding mistakes (wrong labels or vertices)
- try to match mistake graphs with correct ones
- generate the mistake graphs and try to match them all to see which of the generated ones may match at all

**** software 
- graphgrep
- nauty
- see how fast it will take to match

**** From list of vertices
-> generate possible configurations
-> match with database annotated graphs
-> best matching ones go to 2nd step of matching (i.e, decision tree)

**** From this look into training decision tree

**** Start form subgraphs
- Instead of creating entire lobsters initially
- start with smaller subgraphs and match those 
- create the larger graph from subgraphs that are already matched and labelled
- more reliable to detect subgraph rather than entire lobster at once

** Week 5

*** 27/10/17

**** other algorithm
- APPAGTO
- nauty

**** Graphgrep
- try very large dataset to see time taken
- say around 1000 in database
- combinations of all subgraphs
- see if theres any scaling issues
- subgraphs add into the larger lobster graph
- try subgraphs in database with larger query

contrast different approaches

Changes needed from fuse of subgraph to match to larger graph

know that we can find a number of subgraphs in the images
- we will know that subgraphs are valid because matched

next week - define cost fucntion
- different cost functions
- how many nodes found
- angles and distances of edges

*** APPAGATO
**** Approximate network querying
- Find query occurances among all possible with the maximum combined similarity.
- Similarity here means both the similarity between nodes and cost measuring of the differences between nodes and edges

1. Try the APPAGTO algorithm
2. see subgraph matching in graphgrep or nauty -> gives list of matched subgraphs -> cost function

** Week 6
   
*** 03/11/17

**** Matching with errors
- Match subgraphs with errors or variations
- Include filter with probabilities with nodes and edges fitting

**** Add ranking with the probabilities from query
- For each classification of node have an associated probability
- Match all in database
- Naive bayes classifier -> cost of matching (probability the labelling is correct)
    - Probability of each feature and multiply them together
    - When matched the node probabilities will be the same, but combination of nodes/edges and angles will give different probabilities.
    - Gaussian for length matching
- Pick strongest nodes/subgraph from probabilities.



Go back after getting subgraph to get graph properties like edge length and angles.

With list of matched and labelled subgraphs -> building together into whole lobster.

** Week 7

*** 10/11/17

**** Probability model from dataset
- Create probability model from dataset
  - For example create a distribution from distances of labelled nodes
  - If the distances are close between query and database, the probability is greater

- Use Bayes on both assignment (subgraph matching) and distances to get total probability/matching score
  - Is the configuration with the best score the assignment we expect?

- Check how changing assignment probabilities change affect of matching with distance


** Week 9
*** 24/11/17
**** Function to introduce errors
- Systematically try different variations when changing probability or edge lenth values

- pick better scores for overlapping nodes -> until entire graph is complete
- higher heiarchicial subgraphs matching like with nodes
- Combining to complete lobster

- try random sample of triplets for high subgraph matching from ranked list of triplets
-> look at ransac 

- look into subset of all triplet assignments and how they could fit into whole lobster

* Semester 2 - Interim demo
** Pre
- Brief summary of everything that has been done.

Feature detection:
- *Harris*: corners
- *SIFT*: blobs
- *SURF*: blobs
- *FAST*: corners
- *BRIEF*: blobs
- *ORB*: fast and rotated BRIEF

Matching:
- Brute force matching
- FLANN based matching
  - Different index algorithms: LinearIndex, KTreeIndex, KMeansIndex, CompositeIndex, AutotuneIndex

Spatial verification with homography

Recognising and detecting:
- Histogram of Oriented Gradients
- Image pyramids
- Sliding windows

** Week 1
*** Questions:
- For vision, using detectors seem to be create descriptor from machine learning (eg HOG+SVM) to be able to classify image. However, we want to identify features of the lobster rather than classify the image as a lobster to be able to create a graph?


- Use size of keypoints to filter and labelling from expected size
- Save the points and sizes remaining
- Hand label
- Try and match based on labelling
- Size and distance of points from each other
- Compare and evaluate between different images to show all points are consistently similar so we can graph match
- Get pose of lobster

** Week 2
- dithering(imagemagic) -> color histogram to filter out wrong colours
- visual similarity check on keypoints
- probabilities after initial filtering

** Week 3
- Look into compare against previous work for carapace length
- Especially for images that didn't work before
- Robustness against noise and scale
- Work for different scales/sizes

If matching works, can use any other technique to find keypoints

*** Labelling notes
*Body* > 300
80 < *Claw* < 300
*Tail* > 300 (smaller than body)
80 < *Head* < 200

*** Matching algorithm notes
*Matches*: [Query graph]:[DB graph]{(query_node1, db_node1), (query_node2, db_node2), ...}

#+BEGIN_SRC pseudocode
for each match in matches
    query_graph = permutations[match.query_id]
    db_graph = get_db_graph(match.db_id)

    if match_nodes() and match_edges()
        good_matches.add(match)

func get_db_graph(id)
    open(id+".gdf") as graph_file
        return Graph(graph_file)

func match_nodes(node_match_tuples)
    for query_node,target_node in node_match_tuples
        if query_node != target_node
            return false
    return true

func match_edges()
    query_edges = []
    target_edges = []

    for each edge in query_edges
        if query_edge[i] != target_edge[i]
            return false

    return true
#+END_SRC

** Week 4
- Probabilty of triplet by product of node probability and product of edge probability
- Edge probability from normal distribution

- For all triplets that contain the keypoint, which triplet is the strongest
- Then sum once for each node
- Edges are the connection of the strong triplet

** Week 6
- Larger annotated set
- Body-tail classification 
- script threshold parameter against existing dataset
- Include edges in model method
- Contrast with previous work to highlight what was done

*** TODO Use edges in model method
*** TODO Check body-tail classification rate
*** TODO Script to analyse/get threshold parameters

- Issue with histogram method for some keypoints not being detected
  > Try histogram for each label and use higher threshold

*** Meeting
Filter before graphgrep with node/edge
max edge distance as function of node size

-> All annotated histograms into flann for comparison

** Week 7
Monday 2pm
compare previous models that only use size/histogram vs more structured approach
- how well we can recognise parts/labels of the lobster
- cross validate by leaving one out -> 20 fold

*** Work
- Previous histogram method was too harsh even with low threshold
- Use multiple histograms (one for each label) and add all points that match
- Better more points and do more processing since any points lost in filtered cannot be used after
- Have to be careful with being too lenient as that would lead to too much detection with noise

** Spring week 1
Which of juvenile/mature model is more probably for matched image
- predict juvenile/mature as classification

Labelling accuracy with precision/recall

Report:
- Justify decisions to show that i am not just randomly trying things
- Evaluate if decisions work for some (eg, vision detectors, using 3 for permutations instead of 2 or 4)


** Spring week 2
*** Before meeting notes
- Context survey -> background
- Context survey -> previous work
- Results -> keypoint labelling how to plot
- Introduction -> objectives

- Draft


*** Meeting
- Precision/recall on all dataset
- The thing that varies are the thresholds

- Explain why images where graph could not be built


** Week 8 
*** Questions
**** Submission
- images too large to submit
- code/readme 

**** Results
- 

**** Report design
- how to take word count
  - detex
  - words in pdf document


**** Other
- poster


*** Meeting report 
**** General
- Citations use \cite{a,b} rather than \cite{a} \cite{b}

**** Abstract
- Abstract say what the results actually were 

**** Context survey
- Previous work
  - do not say adapting the previous work
  - just using his dataset

**** SE-process ()
- Python performance important but not for scripting aspect
- Not performance is not important as a whole

- Looked into nauty, why not used


- Things in se process like testing, version control, environment, python types
- Because not actual process in this section

**** Methodology
- Keypoint labelled using baysian model rather than using Bayes' theorem

- Renamed "keypoint extraction" to higher level title because it more than just extraction in this step

- Group pipeline into subsections, eg keypoint filtering 

- SImply demonstrated why corner detectors not work, not going into too much detail (we were not surprised it doesn't work)

***** Keypoint filtering
- Define term before being used, eg octaves mentioned before being defined

- Keypoints not ``noisy", define noisy as the keypoint not being relevant

- colour histogram issue but address that it will be dealt with later with the graph models.

***** Node labelling
Simply summary sentence at the beginning before going into detail.
- Unsustainable rewording

probability product equation is naive bayes classifier
- add maths to this subsection



**** Results put same as evaluation
Mention other evaluation that will compared against

- Explain precision/recall may not be best metrics
- no threshold that can change to vary precision/recall as based on ranking
- no inherrent trade off between them
- thresholds that were varied are based on filtering/labelling not which final graph to be matched
- Give F1 score for best precision/recall as table
- can report for each body part or each model/method

mature/juvenile classification evaluation

* Notes

** Graph matching notes
- Label nodes having expected number of edges
eg, body should have 4 edges connected to it
  , arm has 2
  , head/tail/claw has 1

** Histogram matching notes
Follow pyimagesearch guide to utilizing color histograms

** Annotating dataset

1. Get all keypoints with SIFT (no filtering)

2. Write all keypoints into gdf format

3. Manually label important keypoints, removing all other keypoints and annotating edges between keypoints

4. Run ~translate.py~ to create finished ~.gdf~ files and put all finished ~.gdf~ files into one.
4a, all complete ~.gdf~ files are needed for node/edge distributions

5. Put all .gdf files as grapgrep db file by running 
~./ggsxe -b -gfu db.gfu~ where ~db.gfu~ contains all annotated graph files together

6. Use separate db file for mature and juvenile

** Method Pipeline

0. Annotation of dataset for keypoint/node/edge distributions

1. Get keypoints with SIFT

2. Filter with octave 
   - Removes all tiny keypoints that are of no relevance

3-0. Tried filtering with size distributions, but not reliable because of different sized lobsters and not robust against scaling 
3-0a. Also tried filtering with keypoint descriptors, but again not effective, often detects very small keypoints

3. Filter with colour histograms
   a. Histogram for each label from model
       - Too strict filtering means losing data
       - Too lenient filtering means too large combination for next step
       - Too lenient filtering also leads to worse performance against noise

4. Create combinations of triplets and filter with graphgrep
   a. Keypoint labelling for combinations using node size probability
   b. Match triplet subgraphs with graphgrep to eliminate impossible configurations
   c. Remaining triplets assigned probability based on node sizes and distribution of edges with those node labels

5. Re-create full lobster graph from remaining triplet permutations
   a. Keypoint method - find best triplet for each keypoint, use the best one if there is overlap
   b. Model method - use a model with a number of labels that should be filled if possible and find best triplet for each label
   c. Graph method - similar to model method but instead of filling just labels, try to fill a graph by each (node-edge-node)

** Using dataset notes
10 lobsters from each category (mature/juvenile)

Leave one out validation where try to match back to one of the 20 dataset lobsters
